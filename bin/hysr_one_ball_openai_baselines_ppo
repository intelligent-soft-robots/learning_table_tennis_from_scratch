#!/usr/bin/env python3

import os
import sys
import math
import gym
import o80
import pam_interface
from learning_table_tennis_from_scratch.hysr_one_ball_env import HysrOneBallEnv
from lightargs import BrightArgs,FileExists
import tensorflow as tf

import baselines
from baselines.common.vec_env import VecFrameStack, VecNormalize, VecEnv
from baselines.common.vec_env.vec_video_recorder import VecVideoRecorder
from baselines.common.vec_env.dummy_vec_env import DummyVecEnv
from baselines.common.cmd_util import common_arg_parser, parse_unknown_args, make_env
from baselines.common.tf_util import get_session
from baselines import logger
from importlib import import_module

from stable_baselines.common import make_vec_env

class Config:

    def __init__(self,pam_config):
        self.accelerated_time = True
        self.o80_pam_time_step = 0.002
        self.mujoco_time_step = 0.002
        self.algo_time_step = 0.01
        self.target_position = [0.45,2.7,0.17]
        self.reference_posture = [(19900,16000),
                                  (16800,19100),
                                  (18700,17300),
                                  (18000,18000)]
        self.pam_config = pam_config
        self.pressure_min = pam_config.min_pressure()
        self.pressure_max = pam_config.max_pressure()
        self.pressure_change_range = self.pressure_max - self.pressure_min
        self.reward_normalization_constant = 1.0
        self.smash_task = False
        self.rtt_cap = -0.2
        self.nb_dofs = 4
        self.world_boundaries = { "min":(0.0,-1.0,+0.17), # x,y,z
                                  "max":(1.6,3.5,+1.5) }   
        self.record_path = None



def get_alg_module_openai_baselines(alg, submodule=None):
    submodule = submodule or alg
    try:
        # first try to import the alg module from baselines
        alg_module = import_module('.'.join(['baselines', alg, submodule]))
    except ImportError:
        # then from rl_algs
        alg_module = import_module('.'.join(['rl_' + 'algs', alg, submodule]))

    return alg_module

def execute(accelerated,
            ppo_config,
            pam_config,
            record_path):

    env_config = Config(pam_config)
    env_config.accelerated_time = accelerated
    env_config.pam_config = pam_config
    env_config.record_path = record_path
    
    env = make_vec_env(HysrOneBallEnv,env_kwargs={"config":env_config})
    #check_env(env)
    total_timesteps = getattr(ppo_config, "num_timesteps")

    ppo_params = ("gamma", "ent_coef",
                  "lr","cliprange",
                  "vf_coef","max_grad_norm","lam",
                  "nminibatches","noptepochs", "network",
                  "num_layers", "num_hidden", "activation", "nsteps")
    ppo_config = { param:getattr(ppo_config,param)
                for param in ppo_params }
    #model = PPO2(MlpPolicy, env, verbose=1, tensorboard_log="/tmp/ppo2",**ppo_config)
    #model.learn(total_timesteps=1000000)
    #model.save("ppo2_hysr_one_ball")

    alg = "ppo2"

    learn = get_alg_module_openai_baselines(alg).learn
    seed = None

    model = learn(
        env=env,
        seed=seed,
        total_timesteps=total_timesteps,
        **ppo_config
    )
    model.save("ppo2_openai_baselines_hysr_one_ball")
    return model, env


def _configure():
    config = BrightArgs(str("learning table tennis from scratch.\n"+
                            "to be started after start_robots or start_robots_accelerated.\n"+
                            "(in same folder)"))
    config.add_operation("accelerated",
                         "if used, start_robot_accelerated must have been started.")
    config.add_option("pam_config_file",
                      pam_interface.DefaultConfiguration.get_path(),
                      "pam configuration file",
                      str,
                      integrity_checks= [FileExists()])
    config.add_option("record_path",
                     "None",
                     "if not None, then it is assumed to be a valid file path,\n"
                     +"and pressures applied to the robot will be recorded to a corresponding file",
                     str)
    config.add_option("gamma",
                      0.99,
                      "ppo discount factor",
                      float)
    config.add_option("num_timesteps",
                      100000,
                      "ppop number of steps to run",
                      int)
    config.add_option("ent_coef",
                      0.001,
                      "ppo entropy coefficient for the loss calculation",
                      float),
    config.add_option("lr",
                      0.0001,
                      "ppo learning rate",
                      float),
    config.add_option("vf_coef",
                      0.5,
                      "ppo value function coefficient for the loss calculation",
                      float),
    config.add_option("max_grad_norm",
                      0.05,
                      "ppo maximum value for gradient clipping",
                      float)
    config.add_option("lam",
                      0.98,
                      "ppo factor for trade-off of bias vs variance for Generalized Advantage Estimator",
                      float)
    config.add_option("nminibatches",
                      8,
                      "ppo number of training minibatches per update",
                      int)
    config.add_option("noptepochs",
                      32,
                      "ppo number of epoch when optimizing the surrogate",
                      int)
    config.add_option("cliprange",
                      0.2,
                      "ppo clipping parameter",
                      float)
    config.add_option("cliprange_vf",
                      0.2,
                      "ppo clipping parameter for the value function",
                      float)
    config.add_option("network",
                      "mlp",
                      "network type (mlp or cnn)",
                      str)
    config.add_option("num_layers",
                      1,
                      "number of layers",
                      int)
    config.add_option("num_hidden",
                      128,
                      "number of hidden unites",
                      int)
    config.add_option("activation",
                      tf.tanh,
                      "activation function",
                      tf.function)
    config.add_option("nsteps",
                      2048,
                      "nsteps",
                      int)



    change_all=False
    finished  = config.dialog(change_all,sys.argv[1:])
    if not finished:
        return None
    return config


def _run():
    config = _configure()
    if config is None:
        return
    record_path = config.record_path
    if record_path == "None":
        record_path = None
    if record_path is not None:
        if os.path.exists(record_path):
            os.remove(record_path)
    pam_config = pam_interface.JsonConfiguration(config.pam_config_file)
    execute(config.accelerated,
            config,
            pam_config,
            record_path)
    accelerated = False
    if("accelerated" in sys.argv):
        accelerated = True


if __name__ == "__main__":
    _run()
